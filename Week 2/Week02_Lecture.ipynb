{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week02_Lecture.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# <div align=\"center\"><font color='red'>  </font></div>\n",
        "# <div align=\"center\"><font color='red'> COSC 2779/2972 | Deep Learning  </font></div>\n",
        "## <div align=\"center\"> <font color='red'> Week 2 Lectorial Example: **Feed-Forward Neural Networks**</font></div>\n",
        "---"
      ],
      "metadata": {
        "id": "0sI9Eb8CBxvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Function Basics\n",
        "\n",
        "Based on:\n",
        "\n",
        "> \"7 popular activation functions you should know in Deep Learning and how to use them with Keras and TensorFlow 2\" by B. Chen\n"
      ],
      "metadata": {
        "id": "Stnaoes5ywZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "z = np.linspace(-7, 7, 200)\n",
        "\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)"
      ],
      "metadata": {
        "id": "0STOlSqhzCle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid (Logistic)\n",
        "\n",
        "The Sigmoid function (also known as the Logistic function) is one of the most widely used activation function. "
      ],
      "metadata": {
        "id": "Pxu1b9hTzL65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([-5, 5], [-1, -1], 'k--', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-', linewidth=1)\n",
        "plt.plot([-7,7], [1,1], 'k--', linewidth=1)\n",
        "# Plot sigmoid\n",
        "plt.plot(z, sigmoid(z), \"b-\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
        "plt.axis([-7, 7, -0.2, 1.2])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([-5, 5], [-1, -1], 'k--', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-', linewidth=1)\n",
        "plt.plot(z, derivative(sigmoid, z), \"b-\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivative\", fontsize=14)\n",
        "plt.axis([-7, 7, -0.2, 1.2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MH0_c7JVzJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The function is a common S-shaped curve.\n",
        "- The output of the function is centered at 0.5 with a range from 0 to 1.\n",
        "- The function is differentiable. That means we can find the slope of the sigmoid curve at any two points.\n",
        "- The function is monotonic but the function’s derivative is not.\n",
        "\n",
        "The Sigmoid function was introduced to Artificial Neural Networks (ANN) to replace the Step function. It was a key change to ANN architecture because the Step function doesn’t have any gradient to work with Gradient Descent, while the Sigmoid function has a well-defined nonzero derivative everywhere, allowing Gradient Descent to make some progress at every step during training.\n",
        "\n",
        "**Problems with Sigmoid activation function**\n",
        "\n",
        "- Vanishing gradient: looking at the function plot, you can see that when inputs become small or large, the function saturates at 0 or 1, with a derivative extremely close to 0. Thus it has almost no gradient to propagate back through the network, so there is almost nothing left for lower layers.\n",
        "- Computationally expensive: the function has an exponential operation.\n",
        "- The output is not zero centered:\n",
        "\n",
        "**The above problems are of concern when sigmoid is used as activation for hidden layer. However, sigmoid is still used as an activation for the last layer. Why?**\n"
      ],
      "metadata": {
        "id": "Xps0MUxb0Vet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanh\n",
        "\n"
      ],
      "metadata": {
        "id": "VAQtKWf71IrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([-5, 5], [-1, -1], 'k--', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-', linewidth=1)\n",
        "plt.plot([-7,7], [1,1], 'k--', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Tanh activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -1.2, 1.2])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivative\", fontsize=14)\n",
        "plt.axis([-5, 5, -1.2, 1.2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6nyLhX5R1SB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The function is a common S-shaped curve as well.\n",
        "- The difference is that the output of Tanh is zero centered with a range from -1 to 1 (instead of 0 to 1 in the case of the Sigmoid function)\n",
        "- The same as the Sigmoid, this function is differentiable\n",
        "- The same as the Sigmoid, the function is monotonic, but the function’s derivative is not.\n",
        "\n",
        "Tanh has characteristics similar to Sigmoid that can work with Gradient Descent. One important point to mention is that Tanh tends to make each layer’s output more or less centered around 0 and this often helps speed up convergence.\n",
        "\n",
        "**Problems with Tanh activation function**\n",
        "\n",
        "- Vanishing gradient: looking at the function plot, you can see that when inputs become small or large, the function saturates at -1 or 1, with a derivative extremely close to 0. Thus it has almost no gradient to propagate back through the network, so there is almost nothing left for lower layers.\n",
        "- Computationally expensive: the function has an exponential operation."
      ],
      "metadata": {
        "id": "0QzA50Dw1TuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rectified Linear Unit (ReLU)\n",
        "\n",
        "The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep learning. The function returns 0 if the input is negative, but for any positive input, it returns that value back."
      ],
      "metadata": {
        "id": "7v3NoSN218_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 5], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, relu(z), \"b-\", linewidth=2, label=\"ReLU\")\n",
        "plt.grid(True)\n",
        "plt.title(\"ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 5])\n",
        "\n",
        "plt.subplot(1, 2,2)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 5], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, derivative(relu, z), \"b-\", linewidth=2, label=\"ReLU\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivative\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3cdLZx7W1_I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Graphically, the ReLU function is composed of two linear pieces to account for non-linearities. A function is non-linear if the slope isn’t constant. So, the ReLU function is non-linear around 0, but the slope is always either 0 (for negative inputs) or 1 (for positive inputs).\n",
        "- The ReLU function is continuous, but it is not differentiable because its derivative is 0 for any negative input.\n",
        "- The output of ReLU does not have a maximum value (It is not saturated) and this helps Gradient Descent\n",
        "- The function is very fast to compute (Compare to Sigmoid and Tanh)\n",
        "\n",
        "It’s surprising that such a simple function works very well in deep neural networks.\n",
        "\n",
        "**Problem with ReLU**\n",
        "ReLU works great in most applications, but it is not perfect. It suffers from a problem known as the dying ReLU.\n",
        "\n",
        "> *Dying ReLU:\n",
        "During training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU function is 0 when its input is negative.(Hands-on Machine Learning, page 329)*."
      ],
      "metadata": {
        "id": "pyLkdDic2XXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leaky ReLU\n",
        "Leaky ReLU is an improvement over the ReLU activation function. It has all properties of ReLU, plus it will never have dying ReLU problem. "
      ],
      "metadata": {
        "id": "rB69I7A33RnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(z, alpha=0.05):\n",
        "    return np.maximum(alpha * z, z)\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 5], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, leaky_relu(z, 0.05), 'b-', linewidth=2)\n",
        "plt.plot([-5,5], [0,0], 'k-')\n",
        "plt.plot([0,0], [-0.5, 5], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5,5,-0.5, 2])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 5], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, derivative(leaky_relu, z), \"b-\", linewidth=2, label=\"Leaky ReLU\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivative\", fontsize=14)\n",
        "plt.axis([-5,5,-0.05, 1.2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qJdO3aoV3R03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Linear Unit (ELU)\n",
        "Exponential Linear Unit (ELU) is a variation of ReLU with a better output for z < 0"
      ],
      "metadata": {
        "id": "XjxgJmEw3itI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elu(z, alpha=1):\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 5], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, elu(z), 'b-', linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1, -1], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(\"ELU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# coordinate \n",
        "plt.plot([-7, 7], [0, 0], 'k-', linewidth=1)\n",
        "plt.plot([0, 0], [-2.2, 5], 'k-', linewidth=1)\n",
        "# Plot\n",
        "plt.plot(z, derivative(elu, z), \"b-\", linewidth=2, label=\"ELU\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivative\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_gtzxLxy4At7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ELU modified the slope of the negative part of the function.\n",
        "- Unlike the Leaky ReLU and PReLU functions, instead of a straight line, ELU uses a log curve for the negative values.\n",
        "\n",
        "According to the authors, ELU outperformed all the ReLU variants in their experiments.\n",
        "\n",
        "**Problem with ELU**\n",
        "\n",
        "The main drawback of the ELU activation is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time, an ELU network will be slower than a ReLU network.\n",
        "\n"
      ],
      "metadata": {
        "id": "gB5eJfgg4KHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to choose an activation function?\n",
        "\n",
        "We have gone through several different activation functions used in deep learning. When building a model, the selection of activation functions is critical. So which activation function should you use? Here is a general suggestion from the book Hands-on ML\n",
        "\n",
        "> Although your mileage will vary, in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network’s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular, RReLU if your network is over‐fitting, or PReLU if you have a huge training set.\n",
        "\n",
        "Hands-on ML, page 332"
      ],
      "metadata": {
        "id": "rZM0sc6L4dKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Notebook"
      ],
      "metadata": {
        "id": "TDe4w6FUF9Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "mPnNDhX2GAdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup tensorboard"
      ],
      "metadata": {
        "id": "MEWmU5zPGe-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ST5QKaKBUEt"
      },
      "outputs": [],
      "source": [
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Open an embedded TensorBoard viewer\n",
        "%tensorboard --logdir {logdir}/models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to plot the models training history ones training has completed."
      ],
      "metadata": {
        "id": "3QPiQyMQGRdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import cycle\n",
        "def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0]):\n",
        "  cycol = cycle('bgrcmk')\n",
        "  for name, item in history_hold.items():\n",
        "    y_train = item.history[metric]\n",
        "    y_val = item.history['val_' + metric]\n",
        "    x_train = np.arange(0,len(y_val))\n",
        "\n",
        "    c=next(cycol)\n",
        "\n",
        "    plt.plot(x_train, y_train, c+'-', label=name+'_train')\n",
        "    plt.plot(x_train, y_val, c+'--', label=name+'_val')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.xlim([1, max(plt.xlim())])\n",
        "  plt.ylim(ylim)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(metric)\n",
        "  plt.grid(True)"
      ],
      "metadata": {
        "id": "sIzlCknLGYsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create callbacks for tensorboard"
      ],
      "metadata": {
        "id": "JlBcxA_suXbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_histories = {}\n",
        "\n",
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=1),\n",
        "  ]"
      ],
      "metadata": {
        "id": "TvjW9tSFuWpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset \n",
        "\n",
        "Lets load the dataset from the internet and set it up to be used with deep learning models"
      ],
      "metadata": {
        "id": "rOkStgqBGmm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e5g5oujUIscl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/'My Drive'/COSC2779/Lectures/Week02/default_of_credit_card_clients.csv .\n",
        "!ls"
      ],
      "metadata": {
        "id": "E4igwIgTIy_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"default_of_credit_card_clients.csv\")"
      ],
      "metadata": {
        "id": "10O8skNFGmw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "BA4nj7h9JN8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Data\n",
        "\n",
        "## Description of the dataset\n",
        "\n",
        "This UCI dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. The dataset can be found at.\n",
        "\n",
        "The dataset is composed by 24 variables in total. The first variables contains information about the user personal information:\n",
        "\n",
        "- ID: ID of each client, categorical variable\n",
        "- LIMIT_BAL: Amount of given credit in New Taiwan dollars (includes individual and family/supplementary credit)\n",
        "- SEX: Gender, categorical variable (1=male, 2=female)\n",
        "- EDUCATION: level of education, categorical variable (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "- MARRIAGE: Marital status, categorical variable (1=married, 2=single, 3=others)\n",
        "- AGE: Age in years, numerical variable \n",
        "\n",
        "\n",
        "Others variables contains information about the history of past payments, the following attributes track the past monthly payment records, i.e. the delay of the payment referred to a specific month:\n",
        "\n",
        "- PAY_0: Repayment status in September 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_2: Repayment status in August 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_3: Repayment status in July 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_4: Repayment status in June 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_5: Repayment status in May 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_6: Repayment status in April 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "\n",
        "The following attributes instead consider the information related to the amount of bill statement, i.e. a monthly report that credit card companies issue to credit card holders in a specific month:\n",
        "\n",
        "- BILL_AMT1: Amount of bill statement in September, 2005 (New Taiwan dollar)\n",
        "- BILL_AMT2: Amount of bill statement in August, 2005 (New Taiwan dollar)\n",
        "- BILL_AMT3: Amount of bill statement in July, 2005 (New Taiwan dollar)\n",
        "- BILL_AMT4: Amount of bill statement in June, 2005 (New Taiwan dollar)\n",
        "- BILL_AMT5: Amount of bill statement in May, 2005 (New Taiwan dollar)\n",
        "- BILL_AMT6: Amount of bill statement in April, 2005 (New Taiwan dollar)\n",
        "\n",
        "The last variables instead consider the amount of previous payment in a specific month:\n",
        "\n",
        "- PAY_AMT1: Amount of previous payment in September, 2005 (New Taiwan dollar)\n",
        "- PAY_AMT2: Amount of previous payment in August, 2005 (New Taiwan dollar)\n",
        "- PAY_AMT3: Amount of previous payment in July, 2005 (New Taiwan dollar)\n",
        "- PAY_AMT4: Amount of previous payment in June, 2005 (New Taiwan dollar)\n",
        "- PAY_AMT5: Amount of previous payment in May, 2005 (New Taiwan dollar)\n",
        "- PAY_AMT6: Amount of previous payment in April, 2005 (New Taiwan dollar)\n",
        "\n",
        "The variable to predict is given by:\n",
        "\n",
        "- default.payment.next.month: indicate whether the credit card holders are defaulters or non-defaulters (1=yes, 0=no)"
      ],
      "metadata": {
        "id": "5HtK686sJbUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Distribution"
      ],
      "metadata": {
        "id": "lJCEWdaJL-Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "for i, col in enumerate(data.columns):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.hist(data[col], alpha=0.3, color='b', density=True)\n",
        "    plt.title(col)\n",
        "    plt.xticks(rotation='vertical')"
      ],
      "metadata": {
        "id": "xgxLku7HJfrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning\n",
        "\n",
        "The data cleaning process is the procedure of correcting or removing incomplete/inaccurate or incorrect portions of the dataset. "
      ],
      "metadata": {
        "id": "HqI3tnl7Styd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['LIMIT_BAL','SEX', 'EDUCATION', 'MARRIAGE', 'AGE']].describe()"
      ],
      "metadata": {
        "id": "ri6j8RJRSt7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = data['EDUCATION'].value_counts()\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "xzcP2scgUElg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As far as EDUCATION is concerned there are three categories not listed in the description of the dataset provided by the UCI website that corresponds to 0, 5, and 6;\n",
        "\n",
        "These rows are deleted"
      ],
      "metadata": {
        "id": "XnrBv0qXUAuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = (data['EDUCATION'] == 0)|(data['EDUCATION'] == 6)|(data['EDUCATION'] == 5)\n",
        "data = data.drop(data.EDUCATION[m].index.values, axis=0)\n",
        "summary = data['EDUCATION'].value_counts()\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "JiwYGxoCSpwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for MARRIAGE category from the function .describe() it is possible to notice that there is a minimum value equal to 0, that does not corresponds to any category previously described."
      ],
      "metadata": {
        "id": "j8MV2E_MUvI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = data['MARRIAGE'].value_counts()\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "utN8izYjUacu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = (data['MARRIAGE'] == 0)\n",
        "data = data.drop(data.MARRIAGE[m].index.values, axis=0)\n",
        "\n",
        "summary = data['MARRIAGE'].value_counts()\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "i3DKBbjIUhfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[['PAY_' + str(n) for n in [0, 2, 3, 4, 5, 6]]].describe()"
      ],
      "metadata": {
        "id": "VgQYmSjTVEiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As far as the attributes PAY_* is concerned, all of this attributes have a minimum value equal to -2, not included in the ranking. On the other hand the maximum value assumed is equal 8 so probabily it is necessary a re-scaling of the attribute. "
      ],
      "metadata": {
        "id": "cefhqv4JVglH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['PAY_' + str(n) for n in [0, 2, 3, 4, 5, 6]]] += 1"
      ],
      "metadata": {
        "id": "60CwlwM3VilR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = data['MARRIAGE'].value_counts()\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "Wanl8TMfbLjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "FgUdxdKPJhjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## One-hot encoding for categorical variables\n",
        "\n",
        "Categorical variable such as SEX, MARRIAGE and EDUCATION are turned into one-hot variable in order to remove any orders that in this case have no meaning"
      ],
      "metadata": {
        "id": "7DDhqCixIhOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['EDUCATION'] = data['EDUCATION'].astype('category')\n",
        "data['SEX'] = data['SEX'].astype('category')\n",
        "data['MARRIAGE'] = data['MARRIAGE'].astype('category')\n",
        "\n",
        "data=pd.concat([pd.get_dummies(data['EDUCATION'], prefix='EDUCATION'), \n",
        "                  pd.get_dummies(data['SEX'], prefix='SEX'), \n",
        "                  pd.get_dummies(data['MARRIAGE'], prefix='MARRIAGE'),\n",
        "                  data],axis=1)\n",
        "data.drop(['EDUCATION'],axis=1, inplace=True)\n",
        "data.drop(['SEX'],axis=1, inplace=True)\n",
        "data.drop(['MARRIAGE'],axis=1, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "whxwk_LLIgq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Min Max Scaling\n",
        "\n",
        "Input variables may have different units so different scales; for this reason before drawing a boxplot, a MinMaxScaler() is applied in order to scale the features between a range (0, 1). The transformation is given by the following formula:\n",
        "\n",
        "\\begin{equation} X{std} = \\frac{(X - X{min})}{(X{max} - X{min})} \\end{equation}\n",
        "\n",
        "\\begin{equation} X{scaled} = X{std} * (max - min) + min \\end{equation} Where:\n",
        "\n",
        "$X_{min}$ is the minimum value on the column\n",
        "\n",
        "$X_{max}$ is the maximum value on the column\n",
        "\n",
        "$(min, max)$ are the extreme values of the range chosen, in this case $(0, 1)$\n",
        "\n",
        "This transformation is applied on numerical features only as the categorical variables are transformed into one-hot vectors, that rescale the categorical variable in the range $(0,1)$."
      ],
      "metadata": {
        "id": "mjSyLpz1JR6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data['LIMIT_BAL'] = scaler.fit_transform(data['LIMIT_BAL'].values.reshape(-1, 1))\n",
        "data['AGE'] = scaler.fit_transform(data['AGE'].values.reshape(-1, 1))\n",
        "\n",
        "\n",
        "for i in range(1,7):\n",
        "    scaler = MinMaxScaler()\n",
        "    data['BILL_AMT' + str(i)] = scaler.fit_transform(data['BILL_AMT' + str(i)].values.reshape(-1, 1))\n",
        "\n",
        "for i in range(1,7):\n",
        "    scaler = MinMaxScaler()\n",
        "    data['PAY_AMT' + str(i)] = scaler.fit_transform(data['PAY_AMT' + str(i)].values.reshape(-1, 1))\n",
        "    \n",
        "for i in [0, 2, 3, 4, 5, 6]:\n",
        "    scaler = MinMaxScaler()\n",
        "    data['PAY_' + str(i)] = scaler.fit_transform(data['PAY_' + str(i)].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "p9s8wh2cJkQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hold-out Validation\n",
        "\n",
        "In hold out validation we divide the data into 3 subsets:\n",
        "1. Training: to obtaining the parameters or the weights of the hypothesis\n",
        "2. Validation: for tuning hyper-parameters and model selection.\n",
        "3. Test: To evaluate the performance of the developed model. DO NOT use this split to set or tune ant element of the model.\n",
        "\n",
        "For this example lets divide the data into 60/20/20"
      ],
      "metadata": {
        "id": "o9J7eITvZ6JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with pd.option_context('mode.chained_assignment', None):\n",
        "    train_data_, test_data = train_test_split(data, test_size=0.2, shuffle=True,random_state=0)\n",
        "    \n",
        "with pd.option_context('mode.chained_assignment', None):\n",
        "    train_data, val_data = train_test_split(train_data_, test_size=0.25, shuffle=True,random_state=0)\n",
        "    \n",
        "print(train_data.shape[0], val_data.shape[0], test_data.shape[0])"
      ],
      "metadata": {
        "id": "pqqXzxckZ6bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_data.drop(['default payment next month'], axis=1).to_numpy()\n",
        "train_y = train_data[['default payment next month']].to_numpy()\n",
        "\n",
        "test_X = test_data.drop(['default payment next month',], axis=1).to_numpy()\n",
        "test_y = test_data[['default payment next month']].to_numpy()\n",
        "\n",
        "val_X = val_data.drop(['default payment next month',], axis=1).to_numpy()\n",
        "val_y = val_data[['default payment next month']].to_numpy()"
      ],
      "metadata": {
        "id": "5TUM9HfYasIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manage dataset imbalancing\n",
        "\n",
        "In machine learning, it is difficult to train an effective learning model if the class distribution in a given training data set is imbalanced. The overall accuracy may be high, but when computing the accuracy separatly for each class it is possible to notice that the percentage of data points that belongs to the minority class correcly classified is lower than the one computed over the majority class. To tackle this problem one can adopt two different strategies\n",
        "\n",
        "- Oversampling the minority class;\n",
        "- Undersampling the majority class. With the following code it is possible to verify that there is an high imbalance towords the class 0, which is present in almost the 80% of the dataset. This suggest to further adopt a tecnique in order to rebalance the classes."
      ],
      "metadata": {
        "id": "U3h89AUpacOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l0 = len(train_y[train_y == 0])\n",
        "l1 = len(train_y[train_y == 1])\n",
        "\n",
        "print(str(np.round(l0/(l0+l1)*100))+ '%', str(np.round(l1/(l0+l1)*100))+ '%')"
      ],
      "metadata": {
        "id": "yMrImBRYgN2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE\n",
        "\n",
        "The Synthetic Minority Oversampling Technique(SMOTE) generates synthetic points by introducing new observations taken along the line segments joining any or all the points that belong to the class to rebalance."
      ],
      "metadata": {
        "id": "IA0n-Pm-hGTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE , KMeansSMOTE\n",
        "\n",
        "def oversample_dataset(X_train, y_train):\n",
        "\n",
        "    s = f\"<br>Number of instances in the training set before the rebalancing operation: {len(X_train)}\"\n",
        "    #oversample = SMOTE()\n",
        "    oversample = KMeansSMOTE(cluster_balance_threshold=0.00001)\n",
        "    X_train_smote, y_train_smote = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "    s += f\"<br>Number of instances in the training set after the rebalancing operation: {len(X_train_smote)}\"\n",
        "    \n",
        "    l0 = len(y_train_smote[y_train_smote == 0])\n",
        "    l1 = len(y_train_smote[y_train_smote == 1])\n",
        "    \n",
        "    s += f\"<br>There are {l0} rows labelled with 0 ({round(l0/(l1+l0)*100)}%), {l1} rows labelled with 1 ({round(l1/(l1+l0)*100)}%)\"\n",
        "    return X_train_smote, y_train_smote, s"
      ],
      "metadata": {
        "id": "BWJR5UOCacW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_balanced, y_train_balanced, s_balanced = oversample_dataset(X_train = train_X, y_train = train_y)"
      ],
      "metadata": {
        "id": "qMtA4lsNa9J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l0 = len(y_train_balanced[y_train_balanced == 0])\n",
        "l1 = len(y_train_balanced[y_train_balanced == 1])\n",
        "\n",
        "\n",
        "print(str(np.round(l0/(l0+l1)*100))+ '%', str(np.round(l1/(l0+l1)*100))+ '%')"
      ],
      "metadata": {
        "id": "SZ_NZRbhfafm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Modelling"
      ],
      "metadata": {
        "id": "qSSIyfs4KkTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perfomance measures\n",
        "\n",
        "The metrics adopted to evaluate the performance of a classifier are the following:\n",
        "\n",
        "- Accuracy score: it is the ratio of correct predictions (TP$^1$ + TN$^2$) over the total number of data points classified (TP + FN$^4$ + FP$^3$ + TN).\n",
        "- Precision, or positive predictive value, is the number of TP divided by the total number of elements labelled with $1$ (TP + FP), it highlights how valid the results are;\n",
        "- Recall: is the number of TP divided by the total number of elements that actually belong to the positive class (TP + FN), it shows how complete the predictions are;\n",
        "- F-measure, that is the harmonic of precision and recall mean given by the following expression:\n",
        "\\begin{equation} F = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} \\end{equation}\n",
        "\n",
        "As far as accuracy is concerned, in case of class imbalance, this metric may return an high score even if the minority class is not correcly classified. In our case the minority class is the positive one so precision, recall and f-measure are able to measure the goodness of the classifier [10]. In this analysis we focus our attention in detecting which customer may be defaults clients, and the positive class captures the attention of the classifier.\n",
        "\n",
        "The Sklearn function classification_report() returns the results, in terms of accuracy, precision, recall, and f-measure for all the classes considered.\n",
        "\n",
        "$^1$ TP, true positive = the number of items correctly labeled as belonging to the positive class\n",
        "\n",
        "$^2$ TN, true negative = the number of items correctly labeled as belonging to the negative class\n",
        "\n",
        "$^3$ FP, false positive = the number of items wrongly labeled as belonging to the positive class\n",
        "\n",
        "$^2$ FN, false negative = the number of items wrongly labeled as belonging to the negative class\n",
        "\n"
      ],
      "metadata": {
        "id": "IOO7iO2jeeT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(29, )),\n",
        "  tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model1.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'Precision', \n",
        "                       'Recall'])"
      ],
      "metadata": {
        "id": "EQtMjgc0hdib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_histories['simple1H'] = model1.fit(X_train_balanced, y_train_balanced, \n",
        "                                    validation_data=(val_X, val_y), epochs=10, \n",
        "                                    verbose=0, \n",
        "                                    callbacks=get_callbacks('models/simple1H'))"
      ],
      "metadata": {
        "id": "nOh_T2KTiC71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter(m_histories, ylim=[0.0, 1.1], metric = 'accuracy')"
      ],
      "metadata": {
        "id": "yNlkX_nrxDNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_Precision, test_Recall = model1.evaluate(test_X, test_y)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test Precision:', test_Precision)\n",
        "print('Test Recall:', test_Recall)\n",
        "\n"
      ],
      "metadata": {
        "id": "XkWHtxz5k32_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "test_y_pred = model1.predict(test_X)\n",
        "print(classification_report(test_y, test_y_pred>.5))"
      ],
      "metadata": {
        "id": "Va5xlcR_lrgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What should we do next?**"
      ],
      "metadata": {
        "id": "nrMDJRBSxMVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(29, )),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.5),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.5),\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.5),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'Precision', \n",
        "                       'Recall'])\n",
        "\n",
        "m_histories['complex3H'] = model2.fit(X_train_balanced, y_train_balanced, \n",
        "                                    validation_data=(val_X, val_y), epochs=10, \n",
        "                                    verbose=0, \n",
        "                                    callbacks=get_callbacks('models/complex3H'))\n",
        "\n",
        "plotter(m_histories, ylim=[0.0, 1.1], metric = 'accuracy')"
      ],
      "metadata": {
        "id": "Bk-fuLcVx2xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_pred = model2.predict(test_X)\n",
        "print(classification_report(test_y, test_y_pred>.5))"
      ],
      "metadata": {
        "id": "Vfc1WUWDyAsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU vs CPU\n",
        "\n",
        "Make sure you are using a GPU notebook instance. \n",
        "> Edit > notebook settings"
      ],
      "metadata": {
        "id": "v9iLGm2eTuYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "A = np.random.rand(5000, 5000).astype(np.float32)\n",
        "B = np.random.rand(5000, 5000).astype(np.float32)\n",
        "\n",
        "timer = timeit.Timer(\"numpy.dot(A, B)\",\n",
        "\"import numpy; from __main__ import A, B\")\n",
        "numpy_times_list = timer.repeat(10, 1)\n",
        "\n",
        "print('Numpy mean time (s): ', np.mean(numpy_times_list))"
      ],
      "metadata": {
        "id": "Y21wsAxXTx7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "A = tf.convert_to_tensor(A)\n",
        "B = tf.convert_to_tensor(B)\n",
        "\n",
        "timer = timeit.Timer(\"tensorflow.matmul(A, B)\",\n",
        "setup=\"import tensorflow; from __main__ import A, B\")\n",
        "tensorflow_times_list = timer.repeat(10, 1)\n",
        "\n",
        "print('TensorFlow mean time (s): ', np.mean(tensorflow_times_list))"
      ],
      "metadata": {
        "id": "yMOZxetFT24w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}